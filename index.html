<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Yushi Lan</title>

  <meta name="author" content="Yushi Lan">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon"
    href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <!-- <a href="https://clustrmaps.com/site/1c63s"  title="ClustrMaps"><img src="//www.clustrmaps.com/map_v2.png?d=uLEI4e3dpDHqYuj0M071jzmidCzckl28Ogdc5cRNpws&cl=ffffff" /></a> -->
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Yushi Lan</name>
                  </p>
                  <p>
                    I am a postdoctoral researcher at the <a href="https://www.robots.ox.ac.uk/~vgg/">Visual Geometry
                      Group</a>, University of Oxford, working with <a
                      href="https://www.robots.ox.ac.uk/~vedaldi/">Prof. Andrea Vedaldi</a>.

                    I received my Ph.D. from <a href="https://www.ntu.edu.sg/">Nanyang Technological University</a>,
                    advised by <a href="https://www.mmlab-ntu.com/person/ccloy/">Prof. Chen Change Loy</a> and <a
                      href="https://xingangpan.github.io/">Asst. Prof. Xingang Pan</a> at <a
                      href="https://www.mmlab-ntu.com">MMLab@NTU</a>.

                    Earlier, I earned my B.Eng in Software Engineering from Yepeida Honors College, <a
                      href="https://www.bupt.edu.cn">BUPT</a>.

                  </p>
                  <p style="text-align:center">
                    <a href="mailto:lanyushi15@gmail.com">Email</a> &nbsp/&nbsp
                    <a href="data/CV_YushiLan.pdf">CV</a> &nbsp/&nbsp
                    <!-- <a href="data/JonBarron-bio.txt">Bio</a> &nbsp/&nbsp -->
                    <!-- <a href="https://scholar.google.com/citations?hl=en&user=jktWnL8AAAAJ">Google Scholar</a> -->
                    <!-- &nbsp/&nbsp -->
                    <!-- <a href="https://twitter.com/jon_barron">Twitter</a> &nbsp/&nbsp -->
                    <a href="https://github.com/NIRVANALAN/">Github</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">

                  <a href="images/portrait-yslan.JPG"><img style="width:100%;max-width:100%" alt="profile photo"
                      src="images/yslan-indo.jpeg" class="hoverZoomLink"></a>

                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Research</heading>
                  <p>
                    My interests lie in the intersection of computer vision, computer graphics, and machine learning,
                    particularly in inverse graphics powered by neural rendering,
                    including 3D generative models, shape analysis and 3D avatar, etc.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <!-- ! below section TODO, after new paper accepted. -->
          <!-- 
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>News</heading>
                  <p> [2023-06] Joined Google AR as a student researcher, working with <a href="https://www.zhangyinda.com/">Yinda Zhang</a> .</p>
                  <p> [2023-03] Joined Google AR as a student researcher, working with <a href="https://www.zhangyinda.com/">Yinda Zhang</a> .</p>
                </td>
              </tr>
            </tbody>
          </table> -->


          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <!-- iclr 23 -->


              <!-- <div class="teaser-container">
                <img class="teaser" src="/assets/images/EVA3D.gif" alt="EVA3D.gif">
              </div> -->

              <!-- <div class="info-container">

                <p class="title">

                  <a class="title_link" href="https://arxiv.org/abs/2210.04888" target="_blank">EVA3D: Compositional 3D
                    Human Generation from 2D Image Collections
                  </a>

                </p>

                <div class="authors">
                  <a href="https://hongfz16.github.io/" target="_blank">Fangzhou Hong</a>,
                  <a href="https://frozenburning.github.io/" target="_blank"> Zhaoxi Chen</a>,
                  <strong>Yushi Lan</strong>,
                  <a href="https://scholar.google.com/citations?user=lSDISOcAAAAJ&amp;hl=zh-CN" target="_blank">Liang
                    Pan</a>,
                  <a href="https://liuziwei7.github.io" target="_blank">Ziwei Liu</a></p>
                </div>

                <div class="conference">
                  <p><em>International Conference on Learning Representations</em> (<strong>ICLR</strong>), 2023 <span
                      style="color:red;"><strong>(Spotlight)</strong></span></p>
                </div>


                <div class="url">
                  <a href="https://arxiv.org/pdf/2210.04888.pdf" target="_blank">[PDF]</a>
                  <a href="https://hongfz16.github.io/projects/EVA3D.html" target="_blank">[Project Page]</a>
                  <a href="https://www.youtube.com/watch?v=JNV0FJ0aDWM" target="_blank">[Demo]</a>
                  <a href="https://www.unite.ai/creating-full-body-deepfakes-by-combining-multiple-nerfs/"
                    target="_blank">[Press]</a>
                  <a href="https://github.com/hongfz16/EVA3D" target="_blank">[Code]</a>
                  <img src="https://img.shields.io/github/forks/hongfz16/EVA3D?style=social">

                </div>

                <div class="comment">
                  <p>EVA3D is a <strong>high-quality unconditional 3D human generative model</strong> that only requires
                    2D image collections for training.</p>
                </div>

              </div> -->

              <!-- stream3r -->

              <tr onmouseout="e3dge_stop()" onmouseover="e3dge_start()">
                <!-- <tr> -->
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='e3dge_image'><video width=120% height=110% muted autoplay loop>
                        <source src="images/stream3r/dancing_wo_camera.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <!-- <img src='images/gaussian3diff/small_teaser.png' width="160"> -->
                  </div>
                  <!-- <p>A sailboat with mast.</p> -->
                  <!-- <script type="text/javascript">
                    function e3dge_start() {
                      document.getElementById('e3dge_image').style.opacity = "1";
                    }

                    function e3dge_stop() {
                      document.getElementById('e3dge_image').style.opacity = "0";
                    }
                    e3dge_stop()
                  </script> -->
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://nirvanalan.github.io/projects/stream3r/">
                    <papertitle>STream3R: Scalable Sequential 3D Reconstruction
with Causal Transformer
                    </papertitle>
                  </a>
                  <br>

                  <strong>Yushi Lan*</strong>,
                  <a href="https://scholar.google.com/citations?user=fZxK2B0AAAAJ&hl=en">Yihang Luo*</a>,
                  <a href="https://hongfz16.github.io">Fangzhou Hong</a>,
                  <a href="https://shangchenzhou.com/">Shangchen Zhou</a>,
                  <a href="https://chenhonghua.github.io/clay.github.io/">Honghua Chen</a>,
                  <a href="https://zhaoyanglyu.github.io/">Zhaoyang Lyu</a>,
                  <a href="https://williamyang1991.github.io/">Shuai Yang</a>,
                  <a href="https://daibo.info/">Bo Dai</a>,
                  <a href="https://www.mmlab-ntu.com/person/ccloy/">Chen Change Loy</a>
                  <a href="https://xingangpan.github.io/">Xingang Pan</a>
                  <br>
                  <em>arXiv</em>, 2025
                  <br>
                  <a href="https://nirvanalan.github.io/projects/stream3r/">project page</a>
                  /
                  <a href="https://arxiv.org/abs/2508.10893">arXiv</a>
                  /
                  <a href="https://github.com/NIRVANALAN/STream3R">code</a>
                  <!-- /
                  <a href="https://huggingface.co/spaces/yslan/worldmem">demo</a> -->
                  /
                  <a href="https://nirvanalan.github.io/projects/stream3r/static/bibtex.txt">bibtex</a>
                  <p></p>
                  <p>
                    STream3R reformulates dense 3D/4D reconstruction as a sequential registration task with <b>causal attention</b>.
                  </p>
                </td>
              </tr>

              <!-- worldmem -->

              <tr onmouseout="e3dge_stop()" onmouseover="e3dge_start()">
                <!-- <tr> -->
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='e3dge_image'><video width=100% height=100% muted autoplay loop>
                        <source src="https://xizaoqu.github.io/images/worldmem.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <!-- <img src='images/gaussian3diff/small_teaser.png' width="160"> -->
                  </div>
                  <!-- <p>A sailboat with mast.</p> -->
                  <!-- <script type="text/javascript">
                    function e3dge_start() {
                      document.getElementById('e3dge_image').style.opacity = "1";
                    }

                    function e3dge_stop() {
                      document.getElementById('e3dge_image').style.opacity = "0";
                    }
                    e3dge_stop()
                  </script> -->
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://xizaoqu.github.io/worldmem/">
                    <papertitle>WORLDMEM: Long-term Consistent World Simulation with Memory
                    </papertitle>
                  </a>
                  <br>

                  <a href="https://xizaoqu.github.io/">Zeqi Xiao</a>,
                  <strong>Yushi Lan</strong>,
                  <a href="https://zhouyifan.net/about/">Yifan Zhou</a>,
                  <a href="https://vicky0522.github.io/Wenqi-Ouyang">Wenqi Ouyang</a>,
                  <a href="https://williamyang1991.github.io/">Shuai Yang</a>,
                  <a href="https://zengyh1900.github.io/">Yanhong Zeng</a>,
                  <a href="https://xingangpan.github.io/">Xingang Pan</a>
                  <br>
                  <em>arXiv</em>, 2025
                  <br>
                  <a href="https://xizaoqu.github.io/worldmem/">project page</a>
                  /
                  <a href="https://arxiv.org/abs/2504.12369">arXiv</a>
                  /
                  <a href="https://github.com/xizaoqu/WorldMem">code</a>
                  /
                  <a href="https://huggingface.co/spaces/yslan/worldmem">demo</a>
                  /
                  <a href="https://xizaoqu.github.io/data/bib/worldmem.bib">bibtex</a>
                  <p></p>
                  <p>
                    WORLDMEM enables long-term memory for video world models.
                  </p>
                </td>
              </tr>


              <!-- Obj-2.5D -->

              <tr onmouseout="e3dge_stop()" onmouseover="e3dge_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <img
                      src='https://wzhouxiff.github.io/projects/ObjCtrl-2.5D/assets/results/traj/rose_swing_demo_0_0.9_0.5_rescale2_k5_trajectory.gif'
                      width="160">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://wzhouxiff.github.io/projects/ObjCtrl-2.5D/">
                    <papertitle>ObjCtrl-2.5D: Training-free Object Control with Camera Poses
                    </papertitle>
                  </a>
                  <br>

                  <a href="https://wzhouxiff.github.io/">Zhouxia Wang</a>,
                  <strong>Yushi Lan</strong>,
                  <a href="https://shangchenzhou.com/">Shangchen Zhou</a>,
                  <a href="https://www.mmlab-ntu.com/person/ccloy/">Chen Change Loy</a>
                  <br>
                  <em>preprint</em>, 2024
                  <br>
                  <a href="https://wzhouxiff.github.io/projects/ObjCtrl-2.5D/">project page</a>
                  /
                  <a href="https://arxiv.org/pdf/2412.07721">arXiv</a>
                  /
                  <a href="https://github.com/wzhouxiff/ObjCtrl-2.5D">Code</a>
                  /
                  <a href="https://huggingface.co/spaces/yslan/ObjCtrl-2.5D">Demo</a>

                  <p></p>
                  <p>
                    ObjCtrl-2.5D enables training-free, precise, and versatile object control in I2V generation by using
                    3D
                    depth and camera trajectories.
                  </p>
                </td>
              </tr>


              <!-- MVDrag3D -->

              <tr onmouseout="e3dge_stop()" onmouseover="e3dge_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='e3dge_image'><video width=100% height=100% muted autoplay loop>
                        <source src="images/mvdrag3d/mvdrag-sparrow.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                  </div>

                  <!-- <p>A sailboat with mast.</p> -->
                  <!-- <script type="text/javascript">
                    function e3dge_start() {
                      document.getElementById('e3dge_image').style.opacity = "1";
                    }

                    function e3dge_stop() {
                      document.getElementById('e3dge_image').style.opacity = "0";
                    }
                    e3dge_stop()
                  </script> -->

                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://chenhonghua.github.io/MyProjects/MvDrag3D/">
                    <papertitle>MVDrag3D: Drag-based Creative 3D Editing via Multi-view Generation-Reconstruction Priors
                    </papertitle>
                  </a>
                  <br>

                  <a href="https://chenhonghua.github.io/clay.github.io/">Honghua Chen</a>,
                  <strong>Yushi Lan</strong>,
                  <a href="https://cyw-3d.github.io/">Yongwei Chen</a>,
                  <a href="https://zhouyifan.net/">Yifan Zhou</a>,
                  <a href="https://xingangpan.github.io/">Xingang Pan</a>,
                  <br>
                  <em>preprint</em>, 2024
                  <br>
                  <a href="https://github.com/chenhonghua/MvDrag3D">project page</a>
                  /
                  <a href="https://arxiv.org/abs/2410.16272">arXiv</a>
                  /
                  <a href="https://github.com/chenhonghua/MvDrag3D">Code</a>

                  <p></p>
                  <p>
                    MvDrag3D provide a <i>precise, generative, and flexible</i> solution for 3D drag-based editing.
                  </p>
                </td>
              </tr>

              <!-- 3D Morphing -->

              <tr onmouseout="e3dge_stop()" onmouseover="e3dge_start()">
                <!-- <tr> -->
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <!-- <div class="two" id='e3dge_image'><video width=100% height=100% muted autoplay loop>
                        <source src="https://songlin1998.github.io/images/publications/morphing.gif" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div> -->
                    <img src='https://songlin1998.github.io/images/publications/morphing.gif' width=100%>
                  </div>
                  <!-- <p>A sailboat with mast.</p> -->
                  <!-- <script type="text/javascript">
                    function e3dge_start() {
                      document.getElementById('e3dge_image').style.opacity = "1";
                    }

                    function e3dge_stop() {
                      document.getElementById('e3dge_image').style.opacity = "0";
                    }
                    e3dge_stop()
                  </script> -->
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://songlin1998.github.io/Textured-3D-Morphing/">
                    <papertitle>Textured 3D Regenerative Morphing with 3D Diffusion Prior
                    </papertitle>
                  </a>
                  <br>

                  <a href="https://shangchenzhou.com/">Songlin Yang</a>,
                  <strong>Yushi Lan</strong>,
                  <a href="https://chenhonghua.github.io/clay.github.io/">Honghua Chen</a>,
                  <a href="https://xingangpan.github.io/">Xingang Pan</a>
                  <br>
                  <em>ICCV</em>, 2025
                  <br>
                  <a href="https://songlin1998.github.io/Textured-3D-Morphing/">project page</a>
                  /
                  <a href="https://arxiv.org/pdf/2502.14316">arXiv</a>
                  /
                  <a href="https://github.com/Songlin1998/Textured-3D-DiffMorpher">Code</a>
                  <!-- /
                  <a href="https://huggingface.co/spaces/yslan/GaussianAnything-AIGC3D">Demo</a> -->
                  /
                  <a href="data/bib/textured_morph.bib">bibtex</a>
                  <p></p>
                  <p>
                    GaussianAnything generates <i>high-quality</i> and <i>editable</i> surfel Gaussians through a
                    cascaded 3D diffusion pipeline, given single-view images or texts as the conditions.
                  </p>
                </td>
              </tr>


              <!-- GaussianAnything -->

              <tr onmouseout="e3dge_stop()" onmouseover="e3dge_start()">
                <!-- <tr> -->
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='e3dge_image'><video width=100% height=100% muted autoplay loop>
                        <source src="images/GA/GaussianAnything.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <!-- <img src='images/gaussian3diff/small_teaser.png' width="160"> -->
                  </div>
                  <!-- <p>A sailboat with mast.</p> -->
                  <!-- <script type="text/javascript">
                    function e3dge_start() {
                      document.getElementById('e3dge_image').style.opacity = "1";
                    }

                    function e3dge_stop() {
                      document.getElementById('e3dge_image').style.opacity = "0";
                    }
                    e3dge_stop()
                  </script> -->
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://nirvanalan.github.io/projects/GA/">
                    <papertitle>GaussianAnything: Interactive Point Cloud Latent Diffusion for 3D Generation
                    </papertitle>
                  </a>
                  <br>

                  <strong>Yushi Lan</strong>,
                  <a href="https://shangchenzhou.com/">Shangchen Zhou</a>,
                  <a href="https://zhaoyanglyu.github.io/">Zhaoyang Lyu</a>,
                  <a href="https://hongfz16.github.io">Fangzhou Hong</a>,
                  <a href="https://williamyang1991.github.io/">Shuai Yang</a>,
                  <a href="https://daibo.info/">Bo Dai</a>,
                  <a href="https://xingangpan.github.io/">Xingang Pan</a>,
                  <a href="https://www.mmlab-ntu.com/person/ccloy/">Chen Change Loy</a>
                  <br>
                  <em>ICLR</em>, 2025
                  <br>
                  <a href="https://nirvanalan.github.io/projects/GA/">project page</a>
                  /
                  <a href="https://arxiv.org/abs/2411.08033">arXiv</a>
                  /
                  <a href="https://github.com/NIRVANALAN/GaussianAnything">Code</a>
                  /
                  <a href="https://huggingface.co/spaces/yslan/GaussianAnything-AIGC3D">Demo</a>
                  /
                  <a href="data/bib/gaussiananything.bib">bibtex</a>
                  <p></p>
                  <p>
                    GaussianAnything generates <i>high-quality</i> and <i>editable</i> surfel Gaussians through a
                    cascaded 3D diffusion pipeline, given single-view images or texts as the conditions.
                  </p>
                </td>
              </tr>

              <!-- SAR3D -->

              <tr onmouseout="e3dge_stop()" onmouseover="e3dge_start()">
                <!-- <tr> -->
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='e3dge_image'><video width=100% height=100% muted autoplay loop>
                        <source src="https://cyw-3d.github.io/images/SAR3D.mov" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <!-- <img src='images/gaussian3diff/small_teaser.png' width="160"> -->
                  </div>
                  <!-- <p>A sailboat with mast.</p> -->
                  <!-- <script type="text/javascript">
                    function e3dge_start() {
                      document.getElementById('e3dge_image').style.opacity = "1";
                    }

                    function e3dge_stop() {
                      document.getElementById('e3dge_image').style.opacity = "0";
                    }
                    e3dge_stop()
                  </script> -->
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://cyw-3d.github.io/projects/SAR3D/">
                    <papertitle>SAR3D: Autoregressive 3D Object Generation and Understanding via Multi-scale 3D VQVAE
                    </papertitle>
                  </a>
                  <br>

                  <a href="https://cyw-3d.github.io/">Yongwei Chen</a>,
                  <strong>Yushi Lan</strong>,
                  <a href="https://shangchenzhou.com/">Shangchen Zhou</a>,
                  <a href="https://tengfei-wang.github.io/">Tengfei Wang</a>,
                  <a href="https://xingangpan.github.io/">Xingang Pan</a>
                  <br>
                  <em>CVPR</em>, 2025
                  <br>
                  <a href="https://cyw-3d.github.io/projects/SAR3D/">project page</a>
                  /
                  <a href="https://arxiv.org/pdf/2411.16856">pdf</a>
                  /
                  <a href="https://github.com/cyw-3d/SAR3D">Code</a>
                  /
                  <a href="https://youtu.be/70CV2o7c4Fs">Video</a>
                  /
                  <a href="data/bib/sar3d.bib">bibtex</a>

                  <p></p>
                  <p>
                    SAR3D generates and understands 3D object in a <i>scale-level</i> autoregressive way.
                  </p>
                </td>
              </tr>



              <!-- 3D Enhancer -->

              <tr onmouseout="e3dge_stop()" onmouseover="e3dge_start()">
                <!-- <tr> -->
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='e3dge_image'>
                      <img src='https://shangchenzhou.com/assets/img/papers/3DEnhancer.gif' width="180">
                    </div>
                  </div>
                  <!-- <p>A sailboat with mast.</p> -->
                  <!-- <script type="text/javascript">
                    function e3dge_start() {
                      document.getElementById('e3dge_image').style.opacity = "1";
                    }

                    function e3dge_stop() {
                      document.getElementById('e3dge_image').style.opacity = "0";
                    }
                    e3dge_stop()
                  </script> -->
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://yihangluo.com/projects/3DEnhancer/">
                    <papertitle>3DEnhancer: Consistent Multi-View Diffusion for 3D Enhancement
                    </papertitle>
                  </a>
                  <br>

                  <a href="https://scholar.google.com/citations?user=fZxK2B0AAAAJ&hl=en">Yihang Luo</a>,
                  <a href="https://shangchenzhou.com/">Shangchen Zhou</a>,
                  <strong>Yushi Lan</strong>,
                  <a href="https://xingangpan.github.io/">Xingang Pan</a>,
                  <a href="https://www.mmlab-ntu.com/person/ccloy/">Chen Change Loy</a>
                  <br>
                  <em>CVPR</em>, 2025
                  <br>
                  <a href="https://yihangluo.com/projects/3DEnhancer/">project page</a>
                  /
                  <a href="https://arxiv.org/abs/2412.18565">arXiv</a>
                  /
                  <a href="https://github.com/Luo-Yihang/3DEnhancer">Code</a>
                  /
                  <a href="https://youtu.be/N7bfyd7B4D8">Video</a>
                  /
                  <a href="data/bib/enhancer3d.bib">bibtex</a>

                  <p></p>
                  <p>
                    3DEnhancer enhances low-quality 3D assets through multi-view diffusion priors.
                  </p>
                </td>
              </tr>


              <!-- 3DTopia-XL -->

              <tr onmouseout="e3dge_stop()" onmouseover="e3dge_start()">
                <!-- <tr> -->
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='e3dge_image'>
                      <!-- <video width=100% height=100% muted autoplay loop>
                        <source src="images/ln3diff/mast.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video> -->
                      <img src='https://frozenburning.github.io/images/3dtopia-xl.gif' width="160">
                    </div>
                  </div>
                  <!-- <p>A sailboat with mast.</p> -->
                  <!-- <script type="text/javascript">
                    function e3dge_start() {
                      document.getElementById('e3dge_image').style.opacity = "1";
                    }

                    function e3dge_stop() {
                      document.getElementById('e3dge_image').style.opacity = "0";
                    }
                    e3dge_stop()
                  </script> -->
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://3dtopia.github.io/3DTopia-XL/">
                    <papertitle>3DTopia-XL: Scaling High-quality 3D Asset Generation via Primitive Diffusion
                    </papertitle>
                  </a>
                  <br>

                  <a href="https://frozenburning.github.io/">Zhaoxi Chen</a>,
                  <a href="https://me.kiui.moe/">Jiaxiang Tang</a>,
                  <a href="https://scholar.google.com/citations?user=kMui170AAAAJ&hl=zh-CN">Yuhao Dong</a>,
                  <a href="https://ziangcao0312.github.io/">Ziang Cao</a>,
                  <a href="https://hongfz16.github.io">Fangzhou Hong</a>,
                  <strong>Yushi Lan</strong>,
                  <a href="https://tengfei-wang.github.io/">Tengfei Wang</a>,
                  <a href="https://haozhexie.com/about">Haozhe Xie</a>,
                  <a href="https://wutong16.github.io/">Tong Wu</a>,
                  <a href="https://shunsukesaito.github.io/">Shunsuke Saito</a>,
                  <a href="https://scholar.google.com/citations?user=lSDISOcAAAAJ&hl=zh-CN">Liang Pan</a>,
                  <a href="http://dahua.site/">Dahua Lin</a>,
                  <a href="https://liuziwei7.github.io" target="_blank">Ziwei Liu</a>,
                  <br>
                  <em>CVPR</em>, 2025
                  <br>
                  <a href="https://3dtopia.github.io/3DTopia-XL/">project page</a>
                  /
                  <a href="https://arxiv.org/abs/2409.12957">arXiv</a>
                  /
                  <a href="https://github.com/3DTopia/3DTopia-XL">Code</a>

                  <p></p>
                  <p>
                    3DTopia-XL scales high-quality 3D asset generation using Diffusion Transformer (DiT) built upon an
                    expressive and efficient 3D representation, <b>PrimX</b>.
                  </p>
                </td>
              </tr>

              <!-- ln3diff -->

              <tr onmouseout="e3dge_stop()" onmouseover="e3dge_start()">
                <!-- <tr> -->
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='e3dge_image'><video width=100% height=100% muted autoplay loop>
                        <source src="images/ln3diff/ln3diff-teaser.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <!-- <img src='images/gaussian3diff/small_teaser.png' width="160"> -->
                  </div>
                  <!-- <p>A sailboat with mast.</p> -->
                  <!-- <script type="text/javascript">
                    function e3dge_start() {
                      document.getElementById('e3dge_image').style.opacity = "1";
                    }

                    function e3dge_stop() {
                      document.getElementById('e3dge_image').style.opacity = "0";
                    }
                    e3dge_stop()
                  </script> -->
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://nirvanalan.github.io/projects/ln3diff/">
                    <papertitle>LN3Diff: Scalable Latent Neural Fields Diffusion for Speedy 3D Generation
                    </papertitle>
                  </a>
                  <br>

                  <strong>Yushi Lan</strong>,

                  <a href="https://hongfz16.github.io">Fangzhou Hong</a>,
                  <a href="https://williamyang1991.github.io/">Shuai Yang</a>,
                  <a href="https://shangchenzhou.com/">Shangchen Zhou</a>,
                  <a href="https://sg.linkedin.com/in/xuyi-meng-673779208">Xuyi Meng</a>,
                  <a href="https://daibo.info/">Bo Dai</a>,
                  <a href="https://xingangpan.github.io/">Xingang Pan</a>,
                  <a href="https://www.mmlab-ntu.com/person/ccloy/">Chen Change Loy</a>
                  <br>
                  <em>ECCV</em>, 2024
                  <br>
                  <a href="https://nirvanalan.github.io/projects/ln3diff/">project page</a>
                  /
                  <a href="https://arxiv.org/pdf/2403.12019.pdf">arXiv</a>
                  /
                  <a href="https://github.com/NIRVANALAN/LN3Diff">Code</a>
                  /
                  <a href="data/bib/ln3diff.bib">bibtex</a>

                  <p></p>
                  <p>
                    LN3Diff is a native 3D diffusion model that creates high-quality 3D object mesh from image or text
                    within 8 seconds.
                  </p>
                </td>
              </tr>

              <!-- gaussian3diff -->

              <tr onmouseout="e3dge_stop()" onmouseover="e3dge_start()">
                <!-- <tr> -->
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <!-- <div class="two" id='e3dge_image'><video width=100% height=100% muted autoplay loop>
                        <source src="images/dreamfusion.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div> -->
                    <img src='images/gaussian3diff/small_teaser.png' width="160">
                  </div>
                  <!-- <script type="text/javascript">
                    function e3dge_start() {
                      document.getElementById('e3dge_image').style.opacity = "1";
                    }

                    function e3dge_stop() {
                      document.getElementById('e3dge_image').style.opacity = "0";
                    }
                    e3dge_stop()
                  </script> -->
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://nirvanalan.github.io/projects/gaussian3diff/">
                    <papertitle>Gaussian3Diff: 3D Gaussian Diffusion for 3D Full Head Synthesis and Editing
                    </papertitle>
                  </a>
                  <br>

                  <strong>Yushi Lan</strong>,

                  <a href="https://scholar.google.com/citations?hl=en&user=qsrpuKIAAAAJ">Feitong
                    Tan</a>,
                  <a href="https://sylqiu.github.io/">Di Qiu</a>,
                  <a href="https://xharlie.github.io/">Qiangeng Xu</a>
                  <a href="https://www.kylegenova.com/">Kyle Genova</a>
                  <a href="https://zeng.science/">Zeng Huang</a>,
                  <a href="https://www.seanfanello.it/">Sean Fanello</a>,
                  <a href="https://scholar.google.com/citations?user=lJ3VfV8AAAAJ&hl=en">Rohit Pandey</a>,
                  <a href="https://www.cs.princeton.edu/~funk/">Thomas Funkhouser</a>,
                  <a href='https://www.mmlab-ntu.com/person/ccloy/' target='_blank'>Chen Change Loy</a>,
                  <a href="https://www.zhangyinda.com/">Yinda Zhang</a>
                  <br>
                  <em>ECCV</em>, 2024
                  <br>
                  <a href="https://nirvanalan.github.io/projects/gaussian3diff/">project page</a>
                  /
                  <a href="https://arxiv.org/pdf/2312.03763.pdf">arXiv</a>
                  /
                  <a href="data/bib/loc3diff.bib">bibtex</a>

                  <p></p>
                  <p>
                    Gaussian3Diff adopts 3D Gaussians defined in UV space as the underlying 3D
                    representation, which intrinsically support high-quality novel view synthesis, 3DMM-based animation
                    and 3D diffusion for unconditional generation.
                  </p>
                </td>
              </tr>

              <!-- aaai24 -->


              <tr onmouseout="e3dge_stop()" onmouseover="e3dge_start()">
                <!-- <tr> -->
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <!-- <div class="two" id='e3dge_image'><video width=100% height=100% muted autoplay loop>
                        <source src="images/dreamfusion.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div> -->
                    <img src='./images/planedict.png' width="160">
                  </div>
                  <!-- <script type="text/javascript">
                    function e3dge_start() {
                      document.getElementById('e3dge_image').style.opacity = "1";
                    }

                    function e3dge_stop() {
                      document.getElementById('e3dge_image').style.opacity = "0";
                    }
                    e3dge_stop()
                  </script> -->
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://songlin1998.github.io/planedict/">
                    <papertitle>Learning Dense Correspondence for NeRF-Based Face Reenactment
                    </papertitle>
                  </a>
                  <br>

                  <a href="https://songlin1998.github.io/">Songlin Yang</a>,
                  <a href="http://cripac.ia.ac.cn/people/wwang/index.html">Wei Wang</a>,
                  <strong>Yushi Lan</strong>,
                  <a href="https://events.keep.edu.hk/cuhk/engg5700/2017/team/fan-xiangyu/">Xiangyu Fan</a>,
                  <a href="http://cripac.ia.ac.cn/en/EN/column/item139.shtml">Bo Peng</a>,
                  <a href="https://scholar.google.com/citations?hl=zh-CN&amp;user=jZH2IPYAAAAJ">Lei Yang</a>,
                  <a href="http://cripac.ia.ac.cn/people/jdong/">Jing Dong</a>

                  <br>
                  <em>AAAI</em>, 2024
                  <br>
                  <!-- <a href="https://nirvanalan.github.io/projects/gaussian3diff/">project page</a> -->
                  <a href="https://songlin1998.github.io/planedict/">project page</a> /
                  <a href="https://arxiv.org/abs/2312.10422">arXiv</a>

                  <p></p>
                  <p>
                    We propose a novel face reenactment framework,
                    which adopts tri-planes as fundamental NeRF representation and decomposes face tri-planes into three
                    components: canonical tri-planes, identity deformations, and motion.
                  </p>
                </td>
              </tr>


              <!-- iccv23 -->
              <!-- <tr> -->
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='e3dge_image'><video width=100% height=100% muted autoplay loop>
                      <source src="images/dreamfusion.mp4" type="video/mp4">
                      Your browser does not support the video tag.
                    </video></div>
                  <img src='images/deformtoon3d/deformtoon3d.png' width="160">
                </div>
                <!-- <script type="text/javascript">
                  function e3dge_start() {
                    document.getElementById('e3dge_image').style.opacity = "1";
                  }

                  function e3dge_stop() {
                    document.getElementById('e3dge_image').style.opacity = "0";
                  }
                  e3dge_stop()
                </script> -->
              </td>


              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://www.mmlab-ntu.com/project/deformtoon3d/">
                  <papertitle>DeformToon3D: Deformable 3D Toonification from Neural Radiance Fields
                  </papertitle>
                </a>
                <br>

                <a href='https://junzhezhang.github.io/' target='_blank'>Junzhe Zhang*</a>,
                <strong>Yushi Lan*</strong>,
                <a href='https://williamyang1991.github.io/' target='_blank'>Shuai Yang</a>,
                <a href="https://hongfz16.github.io/" target="_blank">Fangzhou Hong</a>,
                <a href="https://scholar.google.com/citations?user=KmxEHm4AAAAJ&hl=zh-TW&citsig=AMD79oqVxlhCocLzrBL2zNFZqVusRRfYow"
                  target="_blank">Quan Wang</a>,
                <a href="https://personal.ntu.edu.sg/asckyeo/" target="_blank">Chai Kiat Yeo</a>,
                <a href="https://liuziwei7.github.io" target="_blank">Ziwei Liu</a>,
                <a href='https://www.mmlab-ntu.com/person/ccloy/' target='_blank'>Chen Change Loy</a>
                <br>
                <em>ICCV</em>, 2023
                <br>
                <a href="https://www.mmlab-ntu.com/project/deformtoon3d/">project page</a>
                /
                <a href="https://arxiv.org/abs/2309.04410">arXiv</a>
                /
                <!-- <a href="https://drive.google.com/file/d/1yDkJfJOLeVlON7ZdRSnR34Ra_ikTVI0A/preview">video</a> -->

                <a href="https://github.com/junzhezhang/DeformToon3D">Code</a>
                /
                <a href="data/bib/deformtoon3d.bib">bibtex</a>

                <p></p>
                <p>
                  We propose DeformToon3D, an 3D toonification methods that achieves high-quality geometry and texture
                  stylization under given styles.
                </p>
              </td>

      </tr>

      <tr onmouseout="e3dge_stop()" onmouseover="e3dge_start()">
        <!-- <tr> -->
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <!-- <div class="two" id='e3dge_image'><video width=100% height=100% muted autoplay loop>
                <source src="images/dreamfusion.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video></div> -->
            <img src='images/E3DGE/e3dge.png' width="160">
          </div>
          <!-- <script type="text/javascript">
            function e3dge_start() {
              document.getElementById('e3dge_image').style.opacity = "1";
            }

            function e3dge_stop() {
              document.getElementById('e3dge_image').style.opacity = "0";
            }
            e3dge_stop()
          </script> -->
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://nirvanalan.github.io/projects/E3DGE/index.html">
            <papertitle>E3DGE: Self-Supervised Geometry-Aware Encoder for Style-Based 3D GAN Inversion
            </papertitle>
          </a>
          <br>

          <strong>Yushi Lan</strong>,
          <a href='' target='_blank'>Xuyi Meng</a>,
          <a href='https://williamyang1991.github.io/' target='_blank'>Shuai Yang</a>,
          <a href='https://www.mmlab-ntu.com/person/ccloy/' target='_blank'>Chen Change Loy</a>,
          <a href='https://daibo.info/' target='_blank'>Bo Dai</a>
          <br>
          <em>CVPR</em>, 2023; <em>IJCV</em>, 2025
          <br>
          <a href="https://nirvanalan.github.io/projects/E3DGE/index.html">project page</a>
          /
          <a href="https://trebuchet.public.springernature.app/get_content/1e6490d8-51c7-48b1-ab20-649243fbf569?utm_source=rct_congratemailt&utm_medium=email&utm_campaign=nonoa_20250614&utm_content=10.1007/s11263-025-02496-2">IJCV</a>
          /
          <a href="https://arxiv.org/abs/2212.07409">arXiv</a>
          /
          <a href="https://drive.google.com/file/d/1yDkJfJOLeVlON7ZdRSnR34Ra_ikTVI0A/preview">video</a>
          /
          <a href="https://github.com/NIRVANALAN/CVPR23-E3DGE">Code</a>
          /
          <a href="data/bib/e3dge.bib">bibtex</a>

          <p></p>
          <p>
            We propose E3DGE, an encoder-based 3D GAN inversion framework that yields high-quality shape and
            texture reconstruction.
          </p>
        </td>
      </tr>



      <!-- eva3d -->
      <tr onmouseout="e3dge_stop()" onmouseover="e3dge_start()">
        <!-- <tr> -->
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='e3dge_image'><video width=100% height=100% muted autoplay loop>
                <source src="images/dreamfusion.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video></div>
            <img src='images/eva3d.gif' width="160">
          </div>
          <!-- <script type="text/javascript">
            function e3dge_start() {
              document.getElementById('e3dge_image').style.opacity = "1";
            }

            function e3dge_stop() {
              document.getElementById('e3dge_image').style.opacity = "0";
            }
            e3dge_stop()
          </script> -->
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://arxiv.org/abs/2210.04888">
            <papertitle>EVA3D: Compositional 3D Human Generation from 2D Image Collections
            </papertitle>
          </a>
          <br>
          <a href="https://hongfz16.github.io/" target="_blank">Fangzhou Hong</a>,
          <a href="https://frozenburning.github.io/" target="_blank"> Zhaoxi Chen</a>,
          <strong>Yushi Lan</strong>,
          <a href="https://scholar.google.com/citations?user=lSDISOcAAAAJ&amp;hl=zh-CN" target="_blank">Liang
            Pan</a>,
          <a href="https://liuziwei7.github.io" target="_blank">Ziwei Liu</a>
          <br>
          <em>ICLR</em>, 2023, <strong>Spotlight</strong>
          <br>
          <a href="https://hongfz16.github.io/projects/EVA3D.html">project page</a>
          /
          <a href="https://arxiv.org/pdf/2210.04888.pdf">arXiv</a>
          /
          <a href="https://www.youtube.com/watch?v=JNV0FJ0aDWM">video</a>
          /
          <a href="https://github.com/hongfz16/EVA3D">Code</a>
          /
          <a href="data/bib/eva3d.bib">bibtex</a>
          <p></p>
          <p>EVA3D is a <strong>high-quality unconditional 3D human generative model</strong> that only requires
            2D image collections for training.</p>
        </td>
      </tr>




      <!-- DDF -->
      <!-- <tr onmouseout="dreamfusion_stop()" onmouseover="dreamfusion_start()" bgcolor="#ffffd0"> -->
      <tr onmouseout="dreamfusion_stop()" onmouseover="dreamfusion_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='dreamfusion_image'><video width=100% height=100% muted autoplay loop>
                <source src="images/dreamfusion.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video></div>
            <img src='images/DDF/deformation.png' width="160">
          </div>
          <!-- <script type="text/javascript">
            function dreamfusion_start() {
              document.getElementById('dreamfusion_image').style.opacity = "1";
            }

            function dreamfusion_stop() {
              document.getElementById('dreamfusion_image').style.opacity = "0";
            }
            dreamfusion_stop()
          </script> -->
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://nirvanalan.github.io/projects/DDF/index.html">
            <papertitle>DDF: Correspondence Distillation from NeRF-Based GAN</papertitle>
          </a>
          <br>
          <strong>Yushi Lan</strong>,
          <a href='https://www.mmlab-ntu.com/person/ccloy/' target='_blank'>Chen Change Loy</a>,
          <a href='https://daibo.info/' target='_blank'>Bo Dai</a>
          <br>
          <em>IJCV</em>, 2022
          <br>
          <a href="https://www.mmlab-ntu.com/project/ddf/index.html">project page</a>
          /
          <a href="https://arxiv.org/abs/2212.09735">arXiv</a>
          /
          <a href="https://rdcu.be/dm0mL">Springer</a>
          /
          <a href="data/bib/ddf.bib">bibtex</a>
          <!-- / -->
          <!-- <a href="https://dreamfusion3d.github.io/gallery.html">gallery</a> -->
          <p></p>
          <p>
            We study dense correspondence, which plays a key role in 3D scene understanding but has been ignored in NeRF
            research.
            DDF presents a novel way to <strong>distill dense NeRF correspondence from pre-trained NeRF GAN
              unsupervisedly.</strong>
          </p>
        </td>
      </tr>

      <!-- MagnifierNet -->
      <!-- <tr onmouseout="dreamfusion_stop()" onmouseover="dreamfusion_start()" bgcolor="#ffffd0"> -->
      <tr onmouseout="dreamfusion_stop()" onmouseover="dreamfusion_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='dreamfusion_image'><video width=100% height=100% muted autoplay loop>
                <source src="images/dreamfusion.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video></div>
            <img src='images/magnifiernet.png' width="160">
          </div>
          <!-- <script type="text/javascript">
            function dreamfusion_start() {
              document.getElementById('dreamfusion_image').style.opacity = "1";
            }

            function dreamfusion_stop() {
              document.getElementById('dreamfusion_image').style.opacity = "0";
            }
            dreamfusion_stop()
          </script> -->
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://nirvanalan.github.io/projects/DDF/index.html">
            <papertitle>Magnifier: Towards Semantic Adversary and Fusion for Person Re-identification</papertitle>
          </a>
          <br>
          <strong>Yushi Lan*</strong>,
          <a href="https://scholar.google.com/citations?user=gERzisMAAAAJ&hl=en" target="_blank">Yuan Liu*</a>,
          <a href="https://scholar.google.com/citations?user=AUFcWtwAAAAJ&hl=en" target="_blank">Xinchi Zhou</a>,
          <a href="https://scholar.google.com.hk/citations?user=UXFCqloAAAAJ&hl=en" target="_blank">Maoqing Tian</a>,
          <a href="https://scholar.google.com/citations?user=vInibgEAAAAJ&hl=en" target="_blank">Xuesen Zhang</a>,
          <a href="https://scholar.google.com/citations?user=afbbNmwAAAAJ&hl=en" target="_blank">Shuai Yi</a>,
          <a href="https://www.ee.cuhk.edu.hk/~hsli/" target="_blank">Hongsheng Li</a>,
          <br>
          <em>BMVC</em>, 2020
          <br>
          <!-- <a href="https://nirvanalan.github.io/projects/DDF/index.html">project page</a>
          / -->
          <a href="https://arxiv.org/abs/2002.10979">arXiv</a>
          /
          <a href="https://github.com/NIRVANALAN/magnifiernet_reid">Code</a>
          <!-- / -->
          <!-- <a href="https://dreamfusion3d.github.io/gallery.html">gallery</a> -->
          <p></p>
          <p>
            We propose MagnifierNet, a triple-branch network which accurately mines details from whole to parts in
            person re-identification (ReID).
          </p>
        </td>
      </tr>


      <!-- <a href="https://clustrmaps.com/site/1c63s"  title="ClustrMaps"><img src="//www.clustrmaps.com/map_v2.png?d=uLEI4e3dpDHqYuj0M071jzmidCzckl28Ogdc5cRNpws&cl=ffffff" /></a> -->



      <!-- template -->
      <!-- <tr onmouseout="dreamfusion_stop()" onmouseover="dreamfusion_start()" bgcolor="#ffffd0">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='dreamfusion_image'><video width=100% height=100% muted autoplay loop>
                        <source src="images/dreamfusion.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <img src='images/dreamfusion.jpg' width="160">
                  </div>
                  <script type="text/javascript">
                    function dreamfusion_start() {
                      document.getElementById('dreamfusion_image').style.opacity = "1";
                    }

                    function dreamfusion_stop() {
                      document.getElementById('dreamfusion_image').style.opacity = "0";
                    }
                    dreamfusion_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://dreamfusion3d.github.io/">
                    <papertitle>DreamFusion: Text-to-3D using 2D Diffusion</papertitle>
                  </a>
                  <br>
                  <a href="https://cs.stanford.edu/~poole/">Ben Poole</a>,
                  <a href="https://www.ajayj.com/">Ajay Jain</a>,
                  <strong>Jonathan T. Barron</strong>,
                  <a href="https://bmild.github.io/">Ben Mildenhall</a>
                  <br>
                  <em>arXiv</em>, 2022
                  <br>
                  <a href="https://dreamfusion3d.github.io/">project page</a>
                  /
                  <a href="https://arxiv.org/abs/2209.14988">arXiv</a>
                  /
                  <a href="https://dreamfusion3d.github.io/gallery.html">gallery</a>
                  <p></p>
                  <p>
                    We optimize a NeRF from scratch using a pretrained text-to-image diffusion model to do text-to-3D
                    generative modeling.
                  </p>
                </td>
              </tr> -->


    </tbody>
  </table>
  <div style="width: 33%; margin: 0 auto;">
    <!-- <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=uLEI4e3dpDHqYuj0M071jzmidCzckl28Ogdc5cRNpws&cl=ffffff&w=a"></script> -->
    <script type='text/javascript' id='clustrmaps'
      src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=a&t=tt&d=uLEI4e3dpDHqYuj0M071jzmidCzckl28Ogdc5cRNpws'></script>
  </div>

  <!-- 
  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tbody>
      <tr>
        <td>
          <heading>Misc</heading>
        </td>
      </tr>
    </tbody>
  </table> -->
  <table width="100%" align="center" border="0" cellpadding="20">
    <!-- <tbody> -->

    <!-- <tr>
                <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/cvf.jpg"></td>
                <td width="75%" valign="center">
                  <a href="https://cvpr2022.thecvf.com/area-chairs">Area Chair, CVPR 2022</a>
                  <br>
                  <a href="http://cvpr2021.thecvf.com/area-chairs">Area Chair & Longuet-Higgins Award Committee Member,
                    CVPR 2021</a>
                  <br>
                  <a href="http://cvpr2019.thecvf.com/area_chairs">Area Chair, CVPR 2019</a>
                  <br>
                  <a href="http://cvpr2018.thecvf.com/organizers/area_chairs">Area Chair, CVPR 2018</a>
                </td> -->
    <!-- </tr> -->
    <!-- <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/cs188.jpg" alt="cs188">
                </td>
                <td width="75%" valign="center">
                  <a href="http://inst.eecs.berkeley.edu/~cs188/sp11/announcements.html">Graduate Student Instructor,
                    CS188 Spring 2011</a>
                  <br>
                  <a href="http://inst.eecs.berkeley.edu/~cs188/fa10/announcements.html">Graduate Student Instructor,
                    CS188 Fall 2010</a>
                  <br>
                  <a href="http://aima.cs.berkeley.edu/">Figures, "Artificial Intelligence: A Modern Approach", 3rd
                    Edition</a>
                </td>
              </tr> -->


    <!-- <tr>
                <td align="center" style="padding:20px;width:25%;vertical-align:middle">
                  <heading>Basically <br> Blog Posts</heading>
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/abs/2112.11687">Squareplus: A Softplus-Like Algebraic Rectifier</a>
                  <br>
                  <a href="https://arxiv.org/abs/2010.09714">A Convenient Generalization of Schlick's Bias and Gain
                    Functions</a>
                  <br>
                  <a href="https://arxiv.org/abs/1704.07483">Continuously Differentiable Exponential Linear Units</a>
                </td>
              </tr> -->


    </tbody>
  </table>

  <table style="width:70%;border:0px;border-spacing:0px;">
    <!-- <table > -->
    <tbody>
      <tr>
        <td style="padding:0px">
          <br>
          <p style="text-align:right;font-size:small;">
            Design and source code from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron's
              website</a>
          </p>
        </td>
      </tr>
    </tbody>
  </table>
  </td>
  </tr>
  </table>
  <!-- <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=a&t=n&d=uLEI4e3dpDHqYuj0M071jzmidCzckl28Ogdc5cRNpws'></script> -->
</body>

</html>